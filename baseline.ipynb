{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ariel ML Challenge Baseline\n",
    "\n",
    "Notebook presenting the baseline model for the [Ariel ML challenge 2021](https://www.ariel-datachallenge.space/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_train_path = ...  # link to training light curves directory\n",
    "params_train_path = ...  # linl to training parameters directory\n",
    "lc_test_path = ...  # link to evaluation light curves directory\n",
    "\n",
    "n_wavelengths = 55\n",
    "n_timesteps = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a Dataset using Pytorch utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ArielMLDataset(Dataset):\n",
    "    def __init__(self, lc_path, params_path=None, transform=None, start_ind=0, \n",
    "                 max_size = int(1e9), shuffle=True, seed=None, device=None):\n",
    "        \"\"\"Dataset to read files for the Ariel ML Data challenge 2021\n",
    "        \n",
    "        Args:\n",
    "            lc_path: str\n",
    "                path to the folder containing the light curves files\n",
    "            params_path: str\n",
    "                path to the folder containing the target transit depths (optional)\n",
    "            transform: callable\n",
    "                transformation to apply to the input light curves\n",
    "            start_ind: int\n",
    "                where to start reading the files from (after ordering)\n",
    "            max_size: int\n",
    "                maximum dataset size\n",
    "            shuffle: bool\n",
    "                whether to shuffle the dataset order or not\n",
    "            seed: int\n",
    "                numpy seed to set in case of shuffling\n",
    "            device: str\n",
    "                torch device\n",
    "        \"\"\"\n",
    "        self.lc_path = lc_path\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "        self.files = sorted([p for p in os.listdir(self.lc_path) if p.endswith('txt')])\n",
    "        if shuffle:\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(self.files)\n",
    "        self.files = self.files[start_ind:start_ind+max_size]\n",
    "\n",
    "        if params_path is not None:                \n",
    "            self.params_path = params_path\n",
    "        else:\n",
    "            self.params_path = None\n",
    "            self.params_files = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_lc_path = Path(self.lc_path) / self.files[idx]\n",
    "        lc = torch.from_numpy(np.loadtxt(item_lc_path))\n",
    "        if self.transform:\n",
    "            lc = self.transform(lc)\n",
    "        if self.params_path is not None:\n",
    "            item_params_path = Path(self.params_path) / self.files[idx]\n",
    "            target = torch.from_numpy(np.loadtxt(item_params_path))\n",
    "        else:\n",
    "            target = torch.Tensor()\n",
    "        return {'lc': lc.to(self.device), \n",
    "                'target': target.to(self.device)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's plot a random spectral light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ArielMLDataset(lc_train_path, params_train_path, shuffle=True)\n",
    "\n",
    "idx = np.random.randint(len(dataset))\n",
    "item = dataset[idx]\n",
    "offsets = np.linspace(-0.05, 0.05, item['lc'].shape[0])\n",
    "f, ax = plt.subplots(figsize=(13,9))\n",
    "plt.plot(item['lc'].T.detach().numpy() + offsets , label=None)\n",
    "ax.legend([round(x, 4) for x in item['target'].detach().numpy()], fontsize=6, loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define simples preprocessing steps\n",
    "- smoothing \n",
    "- clipping\n",
    "- normalisation per wavelength\n",
    "- removing ramp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    \"\"\"Perform a simple preprocessing of the input light curve array\n",
    "    Args:\n",
    "        x: np.array\n",
    "            first dimension is time, at least 30 timesteps\n",
    "    Return:\n",
    "        preprocessed array\n",
    "    \"\"\"\n",
    "    out = x.clone()\n",
    "    # naive croping of main ramp part (replaced by 1.)\n",
    "    out[:30] = 1.\n",
    "    # centering\n",
    "    out -=  1.\n",
    "    # rough rescaling \n",
    "    out /= 0.04\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include these steps in the datasets for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 64  #4 * 128\n",
    "val_size = 64  #4 * 128\n",
    "test_size = 16  #1024\n",
    "\n",
    "# Training\n",
    "dataset_train = ArielMLDataset(lc_train_path, params_train_path, shuffle=True, start_ind=0, \n",
    "                               max_size=train_size, transform=transform)\n",
    "# Validation\n",
    "dataset_val = ArielMLDataset(lc_train_path, params_train_path, shuffle=True, start_ind=train_size, \n",
    "                             max_size=val_size, transform=transform)\n",
    "\n",
    "# Testing\n",
    "dataset_test = ArielMLDataset(lc_train_path, params_train_path, start_ind=train_size+val_size, \n",
    "                              shuffle=True, max_size=test_size, transform=transform)\n",
    "\n",
    "# Evaluation : no output path available here, this will only be used for submission\n",
    "dataset_eval = ArielMLDataset(lc_test_path, shuffle=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the corresponding data loaders, still using Pytorch utils module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size = int(train_size / 4)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size)\n",
    "loader_eval = DataLoader(dataset_eval, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Metric\n",
    "\n",
    "The scoring system used for evaluation is defined here: https://www.ariel-datachallenge.space/ML/documentation/scoring\n",
    "\n",
    "Let's define it here, with unity weights as we don't have the actual weights available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChallengeMetric:\n",
    "    def __init__(self, weights=None):\n",
    "        self.weights = weights\n",
    "        \n",
    "    def __call__(self, y, pred):\n",
    "        y = y\n",
    "        pred = pred\n",
    "        if self.weights is None:\n",
    "            weights = torch.ones_like(y, requires_grad=False)\n",
    "        else:\n",
    "            weights = self.weights\n",
    "\n",
    "        return (weights * y * torch.abs(pred - y)).sum() / weights.sum() * 1e6\n",
    "    \n",
    "    def score(self, y, pred):\n",
    "        y = y\n",
    "        pred = pred\n",
    "        if self.weights is None:\n",
    "            weights = torch.ones_like(y, requires_grad=False)\n",
    "        else:\n",
    "            weights = self.weights\n",
    "\n",
    "        return (1e4 - 2 * (weights * y * torch.abs(pred - y)).sum() / weights.sum() * 1e6)  \n",
    "    \n",
    "challenge_metric = ChallengeMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A constant prediction model for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_1 = lambda x: torch.ones(x.shape[:-1]) * 0.06  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model, a fully connected neural network with 2 hidden layers with ReLU activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Sequential\n",
    "\n",
    "class Baseline(Module):\n",
    "    def __init__(self, H1=1024, H2=256, input_dim=n_wavelengths*n_timesteps, output_dim=n_wavelengths):\n",
    "        super().__init__()\n",
    "        self.network = Sequential(torch.nn.Linear(input_dim, H1),\n",
    "                                  torch.nn.ReLU(),\n",
    "                                  torch.nn.Linear(H1, H2),\n",
    "                                  torch.nn.ReLU(),\n",
    "                                  torch.nn.Linear(H2, output_dim),\n",
    "                                   )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        out = torch.flatten(x, start_dim=1)  # Need to flatten out the input light curves for this type network\n",
    "        out = self.network(out)\n",
    "        return out\n",
    "    \n",
    "baseline = Baseline().double().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "\n",
    "opt = Adam(baseline.parameters())\n",
    "loss_function = MSELoss()\n",
    "# loss_function = ChallengeMetric()\n",
    "# loss_function = L1Loss()\n",
    "\n",
    "challenge_metric = ChallengeMetric()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    val_score = 0\n",
    "    for k, item in enumerate(loader_train):\n",
    "        pred = baseline(item['lc'])\n",
    "        loss = loss_function(item['target'], pred)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()    \n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(loader_train)\n",
    "    for k, item in enumerate(loader_val):\n",
    "        pred = baseline(item['lc'])\n",
    "        loss = loss_function(item['target'], pred)\n",
    "        score = challenge_metric.score(item['target'], pred)\n",
    "        val_loss += loss.item()\n",
    "        val_score += score.item()\n",
    "    val_loss /= len(loader_val)\n",
    "    val_score /= len(loader_val)\n",
    "    print('Training loss', round(train_loss, 6))\n",
    "    print('Val loss', round(val_loss, 6))\n",
    "    print('Val score', round(val_score, 2))\n",
    "    \n",
    "    train_losses += [train_loss]\n",
    "    val_losses += [val_loss]\n",
    "    val_scores += [val_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, '-o', label='Train Loss')\n",
    "plt.plot(val_losses, '-o', label='Val Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel(loss_function)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.plot(val_scores, '-o', label='Val Score')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Challenge score (unity weights)')\n",
    "# plt.yscale('log')\n",
    "plt.ylim(5000,10000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(loader_test))\n",
    "\n",
    "preds = {'naive1': naive_1(item['lc']), \n",
    "         'normal_1000ppm': torch.normal(item['target'], 1e-3),\n",
    "         'baseline': baseline(item['lc'])\n",
    "        }\n",
    "\n",
    "for name, pred in preds.items():\n",
    "    print(name, f\"\\t{challenge_metric(item['target'], pred).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce evaluation vectors\n",
    "(takes a few mins to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tqdm\n",
    "preds = []\n",
    "for k, item in tqdm.tqdm(enumerate(loader_eval)):\n",
    "    preds += [baseline(item['lc'])]\n",
    "\n",
    "eval_pred = torch.cat(preds).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly plot the mean results per wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eval_pred.mean(0), '-o')\n",
    "plt.xlabel('wavelength')\n",
    "plt.ylabel('mean prediction per wavelength')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally save the results as a txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './baseline_evaluation.txt'\n",
    "if save_path and (53900, 55) == eval_pred.shape:\n",
    "    np.savetxt(save_path, eval_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
