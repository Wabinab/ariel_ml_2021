{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ariel_Space_Challenge_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wabinab/ariel_ml_2021/blob/main/Ariel_Space_Challenge_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO03SMrAu_H2"
      },
      "source": [
        "# Ariel Mission Space Challenges\n",
        "By J.W. Chow and R. Cheshire. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "30UGDmypRc4E",
        "outputId": "7f2e26ae-e93e-4db5-fb43-d1aa8c4c6b78"
      },
      "source": [
        "!pip install --upgrade fastavro tensorflow-io google-cloud-bigquery-storage google-cloud-bigquery"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastavro\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/d1/8f5c8611026f0ddcd86a8e2f965998e0c159af980c31efba72342c69f3e4/fastavro-1.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 9.3MB/s \n",
            "\u001b[?25hCollecting tensorflow-io\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/b7/b76c28a422ebaf1c3d97aa6553e8620cc3b0d91976415b4ca255176c7946/tensorflow_io-0.19.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7MB)\n",
            "\u001b[K     |████████████████████████████████| 22.7MB 61.4MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigquery-storage\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/de/7d88e746afade62857f4ebf4adf8048bddfcaceb8e30cc0746d54ee12794/google_cloud_bigquery_storage-2.4.0-py2.py3-none-any.whl (143kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 51.3MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigquery\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/4b/bbc0b268d50a8aec06aa8302b41f9d61c0c8603d16f25ddf7e9542d6b5a1/google_cloud_bigquery-2.20.0-py2.py3-none-any.whl (189kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 57.1MB/s \n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem==0.19.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3a/5c1cc819ff1adfd47fa119a8b904a12207c64bdb1f61f2ef726f03a0cdc6/tensorflow_io_gcs_filesystem-0.19.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tensorflow<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (2.5.0)\n",
            "Requirement already satisfied, skipping upgrade: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery-storage) (1.26.3)\n",
            "Collecting libcst>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/47/ad8ba60c667252be40be06073b58e3234e960ec70608579518e97b368994/libcst-0.3.19-py3-none-any.whl (513kB)\n",
            "\u001b[K     |████████████████████████████████| 522kB 41.2MB/s \n",
            "\u001b[?25hCollecting proto-plus>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/8a/61c5a9b9b6288f9b060b6e3d88374fc083953a29aeac7206616c2d3c9c8e/proto_plus-1.18.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery) (2.23.0)\n",
            "Collecting google-resumable-media<2.0dev,>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/96/4360dc70bef5559b3faf3deeda97aae7d10ff7660d41fd233eb792e7d09f/google_resumable_media-1.3.1-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery) (20.9)\n",
            "Collecting google-cloud-core<2.0dev,>=1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/88/a7/b74266a6fd888d91a6f517c574c17425731181fe44e2df1e414d4b77fbe8/google_cloud_core-1.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (2.5.0)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (2.5.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.34.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.31.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.53.0)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/fa/77565f652ce57ed56d3a63d537c885a18e4451cf2d56d944991aeb3c82bd/typing_inspect-0.7.1-py3-none-any.whl\n",
            "Collecting pyyaml>=5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (1.24.3)\n",
            "Collecting google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/ae/b6efa1019e18c6c791f0f5cd93b2ff40f8f06696dbf04db39ec0f5591b1e/google_crc32c-1.1.2-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-bigquery) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.5.2)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (4.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (0.2.8)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery) (1.14.5)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io) (4.5.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery) (2.20)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io) (3.4.1)\n",
            "\u001b[31mERROR: pandas-gbq 0.13.3 has requirement google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you'll have google-cloud-bigquery 2.20.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-storage 1.18.1 has requirement google-resumable-media<0.5.0dev,>=0.3.1, but you'll have google-resumable-media 1.3.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-bigquery 2.20.0 has requirement google-api-core[grpc]<2.0.0dev,>=1.29.0, but you'll have google-api-core 1.26.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fastavro, tensorflow-io-gcs-filesystem, tensorflow-io, mypy-extensions, typing-inspect, pyyaml, libcst, proto-plus, google-cloud-bigquery-storage, google-crc32c, google-resumable-media, google-cloud-core, google-cloud-bigquery\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: google-cloud-bigquery-storage 1.1.0\n",
            "    Uninstalling google-cloud-bigquery-storage-1.1.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-storage-1.1.0\n",
            "  Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Found existing installation: google-cloud-bigquery 1.21.0\n",
            "    Uninstalling google-cloud-bigquery-1.21.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-1.21.0\n",
            "Successfully installed fastavro-1.4.1 google-cloud-bigquery-2.20.0 google-cloud-bigquery-storage-2.4.0 google-cloud-core-1.7.1 google-crc32c-1.1.2 google-resumable-media-1.3.1 libcst-0.3.19 mypy-extensions-0.4.3 proto-plus-1.18.1 pyyaml-5.4.1 tensorflow-io-0.19.0 tensorflow-io-gcs-filesystem-0.19.0 typing-inspect-0.7.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sisAtTbkRjIR"
      },
      "source": [
        "# Please restart runtime after installation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyyJ9MuMu5SZ"
      },
      "source": [
        "# Code to authorize access to google cloud bucket and mount to drive.  \n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5NhtDTPvICj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3f96cd-dcde-44b6-9d18-aa32256bfc69"
      },
      "source": [
        "# NOTE: Please press enter once, then move cursor from end of line to somewhere middle, \n",
        "# and press enter again, for authentication successfully done. \n",
        "!gcloud auth login"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=CVap1TFAkxJnzr7sQuj0CShoUpHBiU&prompt=consent&access_type=offline&code_challenge=tmeRumlJiq7K84gC3yJShbIMsp6SsFSLi3LYrMzJ7rw&code_challenge_method=S256\n",
            "\n",
            "Enter verification code: 4/1AX4XfWjaergvPyU-ryxTs5Ial7BQTspiQIucSFh-OHCIupxJYwtkYF7sxTM\n",
            "\n",
            "You are now logged in as [chowjunwei37@gmail.com].\n",
            "Your current project is [sunlit-analyst-309609].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS-3I9epve6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa113675-d090-452f-cad4-2de1c2bfb419"
      },
      "source": [
        "PROJECT_ID = \"sunlit-analyst-309609\"\n",
        "!gcloud config set project $PROJECT_ID\n",
        "%env GCLOUD_PROJECT=$PROJECT_ID\n",
        "%load_ext google.cloud.bigquery"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "env: GCLOUD_PROJECT=sunlit-analyst-309609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P5lo1hx5t0C",
        "outputId": "0c63a7c9-c208-4d3d-8a32-c049095a3c7c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 27 01:07:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nviu80wwTeF"
      },
      "source": [
        "**Note: Mounting locally means after unmounting all modifications will be saved into bucket. Careful not to make any amendments of the data inside bucket before creating a backup (to be done).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "JvSAD57w8R-C",
        "outputId": "6228e282-4ff1-4bee-8eca-3bb1900e3462"
      },
      "source": [
        "!pip install google-cloud-storage"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (1.18.1)\n",
            "Collecting google-resumable-media<0.5.0dev,>=0.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/96/d7/b29a41b01b854480891dfc408211ffb0cc7a2a3d5f15a3b6740ec18c845b/google_resumable_media-0.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<0.5.0dev,>=0.3.1->google-cloud-storage) (1.15.0)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (1.26.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage) (4.2.2)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage) (57.0.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (20.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (3.12.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (1.53.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth>=1.2.0->google-cloud-storage) (0.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage) (2.10)\n",
            "\u001b[31mERROR: google-cloud-bigquery 2.20.0 has requirement google-api-core[grpc]<2.0.0dev,>=1.29.0, but you'll have google-api-core 1.26.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-bigquery 2.20.0 has requirement google-resumable-media<2.0dev,>=0.6.0, but you'll have google-resumable-media 0.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-resumable-media\n",
            "  Found existing installation: google-resumable-media 1.3.1\n",
            "    Uninstalling google-resumable-media-1.3.1:\n",
            "      Successfully uninstalled google-resumable-media-1.3.1\n",
            "Successfully installed google-resumable-media-0.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz3a6s1ZwMUs",
        "outputId": "f81a056b-dea9-433b-cd8f-1ca567139a60"
      },
      "source": [
        "# Check where google colab is running so to see if we can prevent network egress charges\n",
        "# Though I don't think we can stop this charge unless we create buckets in multiple locations\n",
        "# and mount the correct one. \n",
        "\n",
        "# The shared bucket I created is not located in whole US but only South Carolina and Iowa\n",
        "# because I'm stupid thinking it might save money that way? And I am not sure how to change\n",
        "# it except for creating a new bucket and transfer the whole bucket from one to another. \n",
        "!curl ipinfo.io"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"ip\": \"34.125.235.16\",\n",
            "  \"hostname\": \"16.235.125.34.bc.googleusercontent.com\",\n",
            "  \"city\": \"Las Vegas\",\n",
            "  \"region\": \"Nevada\",\n",
            "  \"country\": \"US\",\n",
            "  \"loc\": \"36.1750,-115.1372\",\n",
            "  \"org\": \"AS15169 Google LLC\",\n",
            "  \"postal\": \"89111\",\n",
            "  \"timezone\": \"America/Los_Angeles\",\n",
            "  \"readme\": \"https://ipinfo.io/missingauth\"\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW-2sgyp5xfQ",
        "outputId": "b8f2a3ec-4b74-4ec8-f4e2-305fc3d68639"
      },
      "source": [
        "!git clone -b alt_lstm --single-branch https://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@github.com/Wabinab/ariel_ml_2021.git"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ariel_ml_2021'...\n",
            "remote: Enumerating objects: 221, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 221 (delta 64), reused 70 (delta 51), pack-reused 133\u001b[K\n",
            "Receiving objects: 100% (221/221), 4.33 MiB | 10.33 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP3EbcN474Ab"
      },
      "source": [
        "/content/data/test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "v2Dzg_Ad7zja",
        "outputId": "c36774da-2f3a-47e8-bd2e-2df0478b8f51"
      },
      "source": [
        "from ariel_ml_2021.train_baseline import main\n",
        "main()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Failed to load state dict or it did not exist. Training from scratch.\n",
            "epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 5/2400 [01:44<13:51:22, 20.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-412b094d1519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mariel_ml_2021\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_baseline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/ariel_ml_2021/train_baseline.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     np.savetxt(project_dir / f'outputs/train_losses_{prefix}.txt',\n",
            "\u001b[0;32m/content/ariel_ml_2021/train_baseline.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(batch_size, dataset_train, dataset_val, device)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ariel_ml_2021/utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mlc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_lc_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mlc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0;31m# converting the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0mline_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msplit_line\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregex_comments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8BHZHo8wvco",
        "outputId": "08a3f7fd-1dd5-4110-e54f-b66cea4addbf"
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!sudo apt-get -y -q update\n",
        "!sudo apt-get -y -q install gcsfuse\n",
        "!mkdir -p data\n",
        "!gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 arielml_data data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deb http://packages.cloud.google.com/apt gcsfuse-bionic main\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2537  100  2537    0     0    99k      0 --:--:-- --:--:-- --:--:--   99k\n",
            "OK\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:8 http://packages.cloud.google.com/apt gcsfuse-bionic InRelease [5,385 B]\n",
            "Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [599 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:13 http://packages.cloud.google.com/apt gcsfuse-bionic/main amd64 Packages [339 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,418 kB]\n",
            "Hit:16 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:17 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,220 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [473 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,188 kB]\n",
            "Get:22 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,774 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [505 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,653 kB]\n",
            "Get:26 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [907 kB]\n",
            "Get:27 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Get:28 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [41.5 kB]\n",
            "Fetched 13.1 MB in 5s (2,748 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 81 not upgraded.\n",
            "Need to get 10.8 MB of archives.\n",
            "After this operation, 23.1 MB of additional disk space will be used.\n",
            "Get:1 http://packages.cloud.google.com/apt gcsfuse-bionic/main amd64 gcsfuse amd64 0.35.1 [10.8 MB]\n",
            "Fetched 10.8 MB in 1s (11.9 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.35.1_amd64.deb ...\n",
            "Unpacking gcsfuse (0.35.1) ...\n",
            "Setting up gcsfuse (0.35.1) ...\n",
            "2021/06/27 01:07:36.716743 Using mount point: /content/data\n",
            "2021/06/27 01:07:36.724777 Opening GCS connection...\n",
            "2021/06/27 01:07:37.173814 Mounting file system \"arielml_data\"...\n",
            "2021/06/27 01:07:37.207108 File system has been successfully mounted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwEMZO5-xIt0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "bd37b118-47ba-47df-d329-8b343fe06ed9"
      },
      "source": [
        "import os, os.path\n",
        "\n",
        "# Assert correct number of files in case of modifications. \n",
        "\n",
        "assert len(os.listdir(\"./data/test_set/noisy_test\")) == 53900\n",
        "assert len(os.listdir(\"./data/training_set/noisy_train\")) == 125600\n",
        "assert len(os.listdir(\"./data/training_set/params_train\")) == 125600"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2881\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2882\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2883\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-47f5e1117b85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/test_set/noisy_test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m53900\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/training_set/noisy_train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m125600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: './data/test_set/noisy_test'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5_hP7FRSAzh"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tempfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.cloud import bigquery\n",
        "LOCATION = \"us\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9GWu1RYTTJ0"
      },
      "source": [
        "# BigQuery Reader\n",
        "\n",
        "https://www.tensorflow.org/io/tutorials/bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMx-uAbOi0R0"
      },
      "source": [
        "If you want to use the extra params (sun temp, sun logg, sun mag, or whatsoever, use the query below). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUtJ_IOHisgQ"
      },
      "source": [
        "import google.cloud.bigquery.magics\n",
        "google.cloud.bigquery.magics.context.use_bqstorage_api = True  \n",
        "\n",
        "# This allow us to omit the --use_bqstorage_api if this is run. \n",
        "# Although I leave it there just for elegant. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H9vqZLYy11V",
        "outputId": "8bc553b7-e858-45a9-eaa7-45f080b658db"
      },
      "source": [
        "%%bigquery df\n",
        "# SELECT wavelength_num, AAAA, BB, CC, \n",
        "SELECT predicted_label[OFFSET(0)].tables.value,\n",
        "# predicted_label[OFFSET(0)].tables.prediction_interval.start,\n",
        "# predicted_label[OFFSET(0)].tables.prediction_interval.end\n",
        "FROM `sunlit-analyst-309609.prediction_Ariel_ML_retrain_2021_05_31T04_34_18_519Z.predictions`\n",
        "ORDER BY CAST(wavelength_num AS INT64), CAST(AAAA AS INT64), CAST(BB AS INT64), CAST(CC AS INT64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query complete after 0.01s: 100%|██████████| 2/2 [00:00<00:00, 267.23query/s]\n",
            "Downloading: 100%|██████████| 2964500/2964500 [00:02<00:00, 1297631.52rows/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmwY0zxby7zp"
      },
      "source": [
        "numpy_array = df.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubzq3aY8zRLz"
      },
      "source": [
        "new_np_array = numpy_array.reshape((-1, 55))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLa5df6c0O-N"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oULqblEW0Agu"
      },
      "source": [
        "np.savetxt(\"prediction_31052021.txt\", new_np_array, delimiter=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H20xhdw0Tev",
        "outputId": "ba6195b7-6c84-458b-ecfb-4f27e51b7ccd"
      },
      "source": [
        "!gsutil cp prediction_31052021.txt gs://arielml_data/trained_model/31052021/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://prediction_31052021.txt [Content-Type=text/plain]...\n",
            "\\\n",
            "Operation completed over 1 objects/70.7 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKhpujlB0TYN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAOB--ZMYlvC"
      },
      "source": [
        "# Breakline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvavA_YWSaxI"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud import bigquery_storage\n",
        "\n",
        "bqclient = bigquery.Client()\n",
        "bqstorageclient = bigquery_storage.BigQueryReadClient()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNKLIFcREh8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc67921e-c5a0-442b-e9ec-6cec6212d059"
      },
      "source": [
        "###\n",
        "# table_number = 17\n",
        "\n",
        "# query_string = f\"\"\"\n",
        "# SELECT * EXCEPT (AAAA, BB, CC)\n",
        "# FROM `sunlit-analyst-309609.training_set.train_table_{table_number}` a\n",
        "# LEFT JOIN (\n",
        "#     SELECT *\n",
        "#     FROM `sunlit-analyst-309609.training_set.noisy_train_extra_params`\n",
        "# ) b\n",
        "# ON a.AAAA = b.AAAA\n",
        "# \"\"\"\n",
        "\n",
        "# df = (\n",
        "#     bqclient.query(query_string)\n",
        "#     .result()\n",
        "#     .to_dataframe(bqstorage_client=bqstorageclient)\n",
        "# )\n",
        "# print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         _0        _1        _2  ...  star_mass  star_k_mag    period\n",
            "0  0.497010  0.729923  0.808090  ...        1.0      11.547  5.699195\n",
            "1  0.502055  0.714676  0.822699  ...        1.0      11.547  5.699195\n",
            "2  0.505639  0.719042  0.806167  ...        1.0      11.547  5.699195\n",
            "3  0.501602  0.717017  0.808340  ...        1.0      11.547  5.699195\n",
            "4  0.504858  0.726349  0.804311  ...        1.0      11.547  5.699195\n",
            "\n",
            "[5 rows x 307 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62qW4hP3AYmw",
        "outputId": "772d30ee-76a3-4c64-f347-02b5e7c6ed92"
      },
      "source": [
        "# for table_num in tqdm(range(55)):\n",
        "\n",
        "#     query_string = f\"\"\"\n",
        "#     SELECT t.* EXCEPT (rownum)\n",
        "#     FROM \n",
        "#     (\n",
        "#         SELECT * EXCEPT (AAAA, BB, CC), ROW_NUMBER() OVER (ORDER BY a.AAAA, BB, CC) AS rownum \n",
        "#         FROM `sunlit-analyst-309609.training_set.train_table_{table_num}` a\n",
        "#         LEFT JOIN \n",
        "#         (\n",
        "#             SELECT * \n",
        "#             FROM `sunlit-analyst-309609.training_set.noisy_train_extra_params`\n",
        "#         ) b\n",
        "#         ON a.AAAA = b.AAAA\n",
        "#     ) AS t\n",
        "#     WHERE MOD(t.rownum, 223) = 0\n",
        "#     \"\"\"\n",
        "\n",
        "#     df = (\n",
        "#     bqclient.query(query_string)\n",
        "#     .result()\n",
        "#     .to_dataframe(bqstorage_client=bqstorageclient)\n",
        "#     )\n",
        "\n",
        "#     df.to_csv(f\"table_num_{table_num}.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 7/55 [05:46<40:56, 51.17s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evHBKBReTScM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "adc9e97c-4544-4347-d5dd-009bd0f7e688"
      },
      "source": [
        "%%bigquery df --use_bqstorage_api\n",
        "SELECT * EXCEPT (AAAA, BB, CC)\n",
        "FROM `sunlit-analyst-309609.training_set.train_table_0` a\n",
        "LEFT JOIN (\n",
        "    SELECT *\n",
        "    FROM `sunlit-analyst-309609.training_set.noisy_train_extra_params`\n",
        ") b\n",
        "ON a.AAAA = b.AAAA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DefaultCredentialsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4769d679bd37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bigquery'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'df --use_bqstorage_api'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SELECT * EXCEPT (AAAA, BB, CC)\\nFROM `sunlit-analyst-309609.training_set.train_table_0` a\\nLEFT JOIN (\\n    SELECT *\\n    FROM `sunlit-analyst-309609.training_set.noisy_train_extra_params`\\n) b\\nON a.AAAA = b.AAAA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_import_magics.py\u001b[0m in \u001b[0;36mimpl\u001b[0;34m(line, cell, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     return _get_extension_magic(name, module, 'cell',\n\u001b[0;32m---> 88\u001b[0;31m                                 magic_module_loader)(line, cell, **kwargs)\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/magics/magics.py\u001b[0m in \u001b[0;36m_cell_magic\u001b[0;34m(line, query)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_query_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_option_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0mbigquery_client_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery_client_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/magics/magics.py\u001b[0m in \u001b[0;36mproject\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_project\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_project\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_project\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/auth/_default.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_project_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaultCredentialsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_HELP_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mDefaultCredentialsError\u001b[0m: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://cloud.google.com/docs/authentication/getting-started"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq1c9wy3i6mU"
      },
      "source": [
        "If you instead don't need that, use the query below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuRSZaLaiqMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00088f6f-5f1a-4bec-d5be-d995ebe3d882"
      },
      "source": [
        "###\n",
        "# %%bigquery df --use_bqstorage_api\n",
        "# SELECT * EXCEPT (AAAA, BB, CC)\n",
        "# FROM `sunlit-analyst-309609.training_set.train_table_0`"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query complete after 0.02s: 100%|██████████| 2/2 [00:00<00:00, 178.96query/s]\n",
            "Downloading: 100%|██████████| 125600/125600 [00:24<00:00, 5080.26rows/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTlZS528kDKy"
      },
      "source": [
        "### Note on LIMIT\n",
        "\n",
        "Note that here we do not use the LIMIT keyword for BigQuery command because say we ask for LIMIT 1000, that selects the first 1000 rows that meets our query requirements. However, our table are not stored in random form but rather clustered together, that is, clustered according to AAAA. Hence, we will select only the first 10 AAAA's (each with 100 results since there are 10 BB and 10 CC so $10 \\times 10 = 100$ and that is not what we want. \n",
        "\n",
        "Although there are ways of taking more random results such as choosing a result every 50 points or so until we have sufficient results, that would be more difficult than loading the whole dataset up as everyone knows more complex Python but not SQL, which fogs transparency of how it works. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gti_xGedi9KN"
      },
      "source": [
        "Irregardless of both, make sure you **pop** out the **label** field as label (not just select) because the label field is **not the last field so you cannot use column slicing** if you use the **first query**. It is indeed the last field if you use the second query. \n",
        "\n",
        "Note remember to change the name of the dataframe in the query above if want to use that. Currently it stores into a variable named `df_without`, as you can see in the first line. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-Fl_H80UsIp",
        "outputId": "147e2f93-735c-4948-afb9-a8be9ccbee20"
      },
      "source": [
        "# How much memory used in MB. \n",
        "print(df.memory_usage().sum() / (2**20),  \"MB\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "294.1834716796875 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwDy0ruEcwxh"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RgBJ9Fpa7S5"
      },
      "source": [
        "Code below taken from https://www.tensorflow.org/tutorials/load_data/pandas_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26M22lDUbVLR",
        "cellView": "form"
      },
      "source": [
        "batch_size =   20#@param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBUGG0nenpTS"
      },
      "source": [
        "## Metric class\n",
        "I realized that for Keras, perhaps the numerator and denominator should be split apart as we are summing over two different things. And then the final calculation requires us doing it manually, or we could of course assign `model.fit` to a variable in which case that will store the metrics information to be used to plot in a graph. We could use that and take the final epoch value and use that as a calculation. However bear in mind this will be training value instead of test value so beware of how well it generalizes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ_bZ9A4sKIc"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import metrics\n",
        "\n",
        "class ArielChallengeMetricNumerator(metrics.Metric):\n",
        "    def __init__(self, name=\"acm_num\", **kwargs):\n",
        "        super(ArielChallengeMetricNumerator, self).__init__(name=name, **kwargs)\n",
        "        self.weight = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
        "    #y_true=y, y_pred=pred\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):  # score()\n",
        "        \"\"\"\n",
        "        Return the unweighted score related to the challenge. \n",
        "        Taken from https://github.com/ucl-exoplanets/ML-challenge-baseline/blob/main/utils.py\n",
        "\n",
        "        :param y_true: (float) The true label output. \n",
        "        :param y_pred: (float) The predicted output. \n",
        "        \"\"\"\n",
        "\n",
        "        # y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
        "        # values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
        "        # values = tf.cast(values, \"float32\")\n",
        "\n",
        "        y_true = tf.cast(y_true, \"float32\")\n",
        "        y_pred = tf.cast(y_pred, \"float32\")\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
        "        else:\n",
        "            sample_weight = tf.ones_like(y_true, dtype=\"float32\")\n",
        "        \n",
        "        new_weight = 1e4 - (tf.reduce_sum(2 * (sample_weight * y_true * tf.math.abs(y_pred - y_true)), axis=None) / (tf.reduce_sum(sample_weight, axis=None)) * 1e6)\n",
        "        new_weight = tf.reduce_mean(sample_weight * y_true * tf.math.abs(y_pred - y_true))\n",
        "        new_weight = tf.cast(new_weight, \"float32\")\n",
        "        \n",
        "        self.weight.assign_add(new_weight)\n",
        "\n",
        "    def result(self):  # __call__\n",
        "        return self.weight\n",
        "\n",
        "    def reset_state(self):\n",
        "        # The state of the metric will be reset at the start of each epoch.\n",
        "        self.weight.assign(0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmYsOnSXgROG"
      },
      "source": [
        "# OVER HERE!!!\n",
        "I put the model with my normalisation + a few tweaks below, run with batchsize = 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M0bN4rhfyrY"
      },
      "source": [
        "for table_num in range(55):\n",
        "    df = pd.read_csv(f\"table_num_{table_num}.csv\")\n",
        "    mod = abs(df).max()\n",
        "    df /= mod  \n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    try:\n",
        "        del model\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    target = df.pop(\"label\")\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
        "\n",
        "    train_dataset = dataset.cache().shuffle(len(df)).batch(batch_size)\n",
        "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    Dense = tf.keras.layers.Dense\n",
        "    model = tf.keras.Sequential(\n",
        "    [\n",
        "        #306 (incluing extra params for now)\n",
        "        Dense(4, input_shape=(len(df.columns),), activation=tf.nn.relu),    # I exclude this.    \n",
        "        Dense(1064, activation=tf.nn.relu),  # One put the input shape here, for experimentation. \n",
        "        Dense(256, activation=tf.nn.relu),\n",
        "        Dense(1, activation=tf.nn.sigmoid)\n",
        "        ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", \n",
        "        loss = 'mean_squared_error', \n",
        "        metrics=[ArielChallengeMetricNumerator()]) \n",
        "\n",
        "    model.fit(train_dataset, epochs=10, batch_size=batch_size, verbose=2) \n",
        "\n",
        "    print(f\"\\nTable {table_num} finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km5rwYA8X2dw"
      },
      "source": [
        "# New for all 55 wavelengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcHVXFaTX8Yg"
      },
      "source": [
        "#df = \n",
        "mod = abs(df).max()\n",
        "df /= mod\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "try:\n",
        "    del model\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "target = df.pop(\"label\")\n",
        "dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
        "\n",
        "train_dataset = dataset.cache().shuffle(len(df)).batch(batch_size)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "Dense = tf.keras.layers.Dense\n",
        "model = tf.keras.Sequential([\n",
        "    Dense(1064, input_shape=(55*len(df.columns),), activation=tf.nn.relu),\n",
        "    Dense(256, activation=tf.nn.relu),\n",
        "    Dense(55, activation=tf.nn.sigmoid)\n",
        "    ])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\", \n",
        "    loss = 'mean_squared_error', \n",
        "    metrics=[ArielChallengeMetricNumerator()]) \n",
        "\n",
        "model.fit(train_dataset, epochs=10, batch_size=batch_size, verbose=2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVP9KwwbdB4R"
      },
      "source": [
        "## Data Normalization\n",
        "Is this the correct way? I am using https://www.tensorflow.org/tutorials/structured_data/time_series#normalize_the_data\n",
        "\n",
        "Or (should) we use `sklearn.preprocessing.normalize(df)` instead? Or perhaps (weirdly) even with `sklearn.preprocessing.MinMaxScaler` if to some avail? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01zF_k6jbnXu"
      },
      "source": [
        "## For running model individually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urhRC4EYRwcf"
      },
      "source": [
        "#try this normalisation\n",
        "mod = abs(df).max()\n",
        "df /= mod"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8HwCWicWFxx"
      },
      "source": [
        "#msk = np.random.rand(len(df)) < 0.9\n",
        "#dfa = df[msk]\n",
        "#dfb = df[~msk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqcVpfeeMB8x"
      },
      "source": [
        "#prep data\n",
        "target = df.pop(\"label\")\n",
        "dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
        "train_dataset = dataset.cache().shuffle(len(df)).batch(batch_size)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#target = dfb.pop(\"label\")\n",
        "#dataset = tf.data.Dataset.from_tensor_slices((dfb.values, target.values))\n",
        "#val_dataset = dataset.cache().shuffle(len(dfb)).batch(batch_size)\n",
        "#val_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W47gN54_KuQM",
        "outputId": "bf700682-c28d-453d-db4b-2a7da3fa64c7"
      },
      "source": [
        "#model\n",
        "tf.keras.backend.clear_session()\n",
        "try:\n",
        "    del model\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "Dense = tf.keras.layers.Dense\n",
        "model = tf.keras.Sequential(\n",
        "[\n",
        "    #306 (incluing extra params for now)\n",
        "    Dense(4, input_shape=(len(df.columns),), activation=tf.nn.relu),    # I exclude this.    \n",
        "    Dense(1064, activation=tf.nn.relu),  # One put the input shape here, for experimentation. \n",
        "    Dense(256, activation=tf.nn.relu),\n",
        "    Dense(1, activation=tf.nn.sigmoid)\n",
        "    ])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\", \n",
        "    loss = 'mean_squared_error', \n",
        "    metrics=[ArielChallengeMetricNumerator()]) \n",
        "\n",
        "model.fit(train_dataset, epochs=10, batch_size=batch_size, verbose=2) #,validation_data=val_dataset "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "6280/6280 - 25s - loss: 0.0135 - ariel_challenge_metric_num: 136.2193\n",
            "Epoch 2/10\n",
            "6280/6280 - 26s - loss: 0.0088 - ariel_challenge_metric_num: 101.2942\n",
            "Epoch 3/10\n",
            "6280/6280 - 23s - loss: 0.0051 - ariel_challenge_metric_num: 71.0296\n",
            "Epoch 4/10\n",
            "6280/6280 - 23s - loss: 0.0043 - ariel_challenge_metric_num: 64.6431\n",
            "Epoch 5/10\n",
            "6280/6280 - 23s - loss: 0.0038 - ariel_challenge_metric_num: 59.9941\n",
            "Epoch 6/10\n",
            "6280/6280 - 20s - loss: 0.0036 - ariel_challenge_metric_num: 58.1667\n",
            "Epoch 7/10\n",
            "6280/6280 - 20s - loss: 0.0038 - ariel_challenge_metric_num: 59.1862\n",
            "Epoch 8/10\n",
            "6280/6280 - 18s - loss: 0.0033 - ariel_challenge_metric_num: 55.3010\n",
            "Epoch 9/10\n",
            "6280/6280 - 18s - loss: 0.0031 - ariel_challenge_metric_num: 53.2941\n",
            "Epoch 10/10\n",
            "6280/6280 - 18s - loss: 0.0031 - ariel_challenge_metric_num: 52.4903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7facaa3e1d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuwl_YpzXOCr",
        "outputId": "6d773e9f-1f63-4da5-c592-37e445ba1ec9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 10)                3070      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1064)              11704     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               272640    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 287,671\n",
            "Trainable params: 287,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftRZL35_oKe2"
      },
      "source": [
        "The following is actually redundant because we are using only the `sample_weight` which actually doesn't change over the epoch because it is always None so it will always be assigned to `tf.ones_like` section. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LX2jWD3K1hc"
      },
      "source": [
        "I changed the below first layer. I exclude the first layer for experimentation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3EZTgWbbfrB"
      },
      "source": [
        "## For running model in loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xzW0TzHcw9Z"
      },
      "source": [
        "for table_num in range(55):\n",
        "    df = pd.read_csv(f\"table_num_{table_num}.csv\")\n",
        "\n",
        "    target = df.pop(\"label\")\n",
        "\n",
        "    train_mean = df.mean()\n",
        "    train_std = df.std()\n",
        "\n",
        "    df = (df - train_mean) / train_std\n",
        "\n",
        "    target_mean = target.mean()\n",
        "    target_std = target.std()\n",
        "\n",
        "    target = (target - target_mean) / target_std\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    try:\n",
        "        del model\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
        "\n",
        "    train_dataset = dataset.cache().shuffle(len(df)).batch(batch_size)\n",
        "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    Dense = tf.keras.layers.Dense\n",
        "    model = tf.keras.Sequential(\n",
        "    [\n",
        "        #306 (incluing extra params for now)\n",
        "        Dense(10, input_shape=(len(df.columns),), activation=tf.nn.relu),    # I exclude this.    \n",
        "        Dense(1064, activation=tf.nn.relu),  # One put the input shape here, for experimentation. \n",
        "        Dense(256, activation=tf.nn.relu),\n",
        "        Dense(1, activation=tf.nn.sigmoid)\n",
        "        ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", \n",
        "        loss = 'mean_squared_error', \n",
        "        metrics=[ArielChallengeMetricNumerator()]) \n",
        "\n",
        "    model.fit(train_dataset, epochs=13, batch_size=batch_size, verbose=2) \n",
        "\n",
        "    print(f\"\\nTable {table_num} finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtbHc6Oqohg4"
      },
      "source": [
        "Unfortunately with validation it gives some error. I haven't got the patience to fix it yet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOs9mFA3czC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6fe07c-c9a6-4664-8a58-715f29571fcf"
      },
      "source": [
        "    #validation_data=(val, val_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/13\n",
            "12560/12560 [==============================] - 47s 4ms/step - loss: 0.5479 - ariel_challenge_metric_num: 1974.2610 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 2/13\n",
            "   29/12560 [..............................] - ETA: 46s - loss: 0.6218 - ariel_challenge_metric_num: 8.9491 - ariel_challenge_metric_denom: 29.0000 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric ArielChallengeMetricNumerator implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric ArielChallengeMetricDenominator implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12560/12560 [==============================] - 45s 4ms/step - loss: 0.5398 - ariel_challenge_metric_num: 1899.8792 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 3/13\n",
            "12560/12560 [==============================] - 52s 4ms/step - loss: 0.5377 - ariel_challenge_metric_num: 1876.5775 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 4/13\n",
            "12560/12560 [==============================] - 46s 4ms/step - loss: 0.5368 - ariel_challenge_metric_num: 1869.2441 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 5/13\n",
            "12560/12560 [==============================] - 47s 4ms/step - loss: 0.5362 - ariel_challenge_metric_num: 1859.8292 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 6/13\n",
            "12560/12560 [==============================] - 46s 4ms/step - loss: 0.5353 - ariel_challenge_metric_num: 1851.4851 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 7/13\n",
            "12560/12560 [==============================] - 47s 4ms/step - loss: 0.5343 - ariel_challenge_metric_num: 1838.8728 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 8/13\n",
            "12560/12560 [==============================] - 47s 4ms/step - loss: 0.5330 - ariel_challenge_metric_num: 1826.4452 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 9/13\n",
            "12560/12560 [==============================] - 47s 4ms/step - loss: 0.5326 - ariel_challenge_metric_num: 1824.2603 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 10/13\n",
            "12560/12560 [==============================] - 47s 4ms/step - loss: 0.5326 - ariel_challenge_metric_num: 1824.9128 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 11/13\n",
            "12560/12560 [==============================] - 50s 4ms/step - loss: 0.5321 - ariel_challenge_metric_num: 1820.9039 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 12/13\n",
            "12560/12560 [==============================] - 49s 4ms/step - loss: 0.5313 - ariel_challenge_metric_num: 1812.1385 - ariel_challenge_metric_denom: 12560.0000\n",
            "Epoch 13/13\n",
            "12560/12560 [==============================] - 49s 4ms/step - loss: 0.5313 - ariel_challenge_metric_num: 1813.0532 - ariel_challenge_metric_denom: 12560.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc79d0f0290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4zNVcMdaWHS",
        "outputId": "fbb88de6-7b67-4a03-a608-a94840fbc2be"
      },
      "source": [
        "# %%bigquery df_test --use_bqstorage_api\n",
        "# SELECT * EXCEPT (AAAA, BB, CC)\n",
        "# FROM `sunlit-analyst-309609.test_set.test_table_5` a\n",
        "# LEFT JOIN (\n",
        "#     SELECT *\n",
        "#     FROM `sunlit-analyst-309609.test_set.noisy_test_extra_param`\n",
        "# ) b\n",
        "# ON a.AAAA = b.AAAA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query complete after 0.01s: 100%|██████████| 1/1 [00:00<00:00, 175.57query/s]\n",
            "Downloading: 100%|██████████| 53900/53900 [00:12<00:00, 4489.27rows/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mWLuUCWay9o"
      },
      "source": [
        "query_string = f\"\"\"\n",
        "SELECT * EXCEPT (AAAA, BB, CC)\n",
        "FROM `sunlit-analyst-309609.test_set.test_table_{table_number}` a\n",
        "LEFT JOIN (\n",
        "    SELECT *\n",
        "    FROM `sunlit-analyst-309609.test_set.noisy_test_extra_param`\n",
        ") b\n",
        "ON a.AAAA = b.AAAA\n",
        "ORDER BY a.AAAA, BB, CC\n",
        "\"\"\"\n",
        "\n",
        "df_test = (\n",
        "    bqclient.query(query_string)\n",
        "    .result()\n",
        "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgTB_tOsbfQM"
      },
      "source": [
        "df_test = (df_test - train_mean) / train_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tvYMYJvboVw",
        "outputId": "1c21a8df-611e-4e9a-c1aa-e9be89f75330"
      },
      "source": [
        "df_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53900, 306)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LruDut23bHxM"
      },
      "source": [
        "what_is_this = model.predict(df_test, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC2RMlowbXEy",
        "outputId": "f25b03d4-d551-4362-bb30-aa1c344dcfaf"
      },
      "source": [
        "what_is_this.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53900, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eanFQ2A5g3Dd"
      },
      "source": [
        "what_is_this = (what_is_this * target_std) + target_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYODADHthVe5",
        "outputId": "337ae61c-ccc2-423e-c582-d6c40a78ba1f"
      },
      "source": [
        "np.unique(what_is_this).shape  # Find the number of unique values. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11172,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4ray7Sui1it",
        "outputId": "a56f3a35-5558-4d1f-8591-572c926b94b9"
      },
      "source": [
        "min(what_is_this)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.05132152], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYH9dECPjW1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ba67dc-0b02-40b5-8778-f93702b0aebf"
      },
      "source": [
        "max(what_is_this)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.08886544], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeoWG4b3hl4_"
      },
      "source": [
        "Our original is every star have a single unique value. So for testing we have half the number of star we have at training, about 539, should have that many unique value only. But we have 24470 unique values due to probability. Perhaps this might be accurate, we don't know until we hand it in. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgrQCu2U9Ry0"
      },
      "source": [
        "# To unmount after use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BSMZJu7yBgl"
      },
      "source": [
        "# To unmount after use, \n",
        "!fusermount -u data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ8IDjUHa0qV"
      },
      "source": [
        "A = np.loadtxt(\"./data/trained_model/27052021/prediction_27052021.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpxYSEnUe2-M"
      },
      "source": [
        "def update_state(y_true, y_pred, sample_weight=1):  # score()\n",
        "        \"\"\"\n",
        "        Return the unweighted score related to the challenge. \n",
        "        Taken from https://github.com/ucl-exoplanets/ML-challenge-baseline/blob/main/utils.py\n",
        "\n",
        "        :param y_true: (float) The true label output. \n",
        "        :param y_pred: (float) The predicted output. \n",
        "        \"\"\"\n",
        "\n",
        "        # y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
        "        # values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
        "        # values = tf.cast(values, \"float32\")\n",
        "\n",
        "        y_true = tf.cast(y_true, \"float32\")\n",
        "        y_pred = tf.cast(y_pred, \"float32\")\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
        "        else:\n",
        "            sample_weight = tf.ones_like(y_true, dtype=\"float32\")\n",
        "        \n",
        "        new_weight = 1e4 - (tf.reduce_sum(2 * (sample_weight * y_true * tf.math.abs(y_pred - y_true)), axis=None) / (tf.reduce_sum(sample_weight, axis=None)) * 1e6)\n",
        "        # new_weight = tf.reduce_mean(sample_weight * y_true * tf.math.abs(y_pred - y_true))\n",
        "        new_weight = tf.cast(new_weight, \"float32\")\n",
        "        \n",
        "        # self.weight.assign_add(new_weight)\n",
        "        return new_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYiig0PtbKyC"
      },
      "source": [
        "update_state(A, B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DRZbVtkbMTz"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXp4_nCHbOZz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}