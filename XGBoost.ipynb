{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating XGBoost model. \n",
    "\n",
    "This is based on how we trained 55 separate models for 55 separate wavelengths. \n",
    "\n",
    "Note that the difference, other than using XGBoost model, is that Decision Trees does **not** require normalizing data. Hence we can go as it is. \n",
    "\n",
    "And since our data is pre-cleaned, we also do not need to put a pipeline into it. \n",
    "\n",
    "It is a plus that Decision Trees (and hence XGBoost) works best when the features are a collection of categorical and numerical features, OR purely numerical features, which the latter is for ours. \n",
    "\n",
    "And it's a plus if the number of features is far less than the number of training samples. We can drop features as well later during training randomly. \n",
    "\n",
    "However, since we are not familiar with XGBoost, and we have lots of features, tuning it is something of a requirement due to inexperience. We would do bayesian optimization for ourselves. Even though wandb offers pre-configured and easily sent job, we could learn more by implementing ourselves plus I have no idea how to retrieve best parameters from Weights and Biases. \n",
    "\n",
    "Also note that there's a chance we might not use the whole dataset for hparams tuning if it takes too long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GCLOUD_PROJECT=sunlit-analyst-309609\n"
     ]
    }
   ],
   "source": [
    "storage_name = \"baseline_xgboost_pred_1.txt\"\n",
    "\n",
    "PROJECT_ID = \"sunlit-analyst-309609\"\n",
    "%env GCLOUD_PROJECT = $PROJECT_ID\n",
    "%load_ext google.cloud.bigquery\n",
    "\n",
    "# !export GOOGLE_APPLICATION_CREDENTIALS=\"/workspace/ariel_ml_2021/sunlit-analyst-309609-77b8e2f94cb5.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tempfile\n",
    "\n",
    "import ast \n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import xgboost as xgb \n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from google.cloud import bigquery, bigquery_storage\n",
    "LOCATION = \"us\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples taken from https://github.com/fmfn/BayesianOptimization/blob/master/examples/sklearn_example.py and https://www.kdnuggets.com/2019/07/xgboost-random-forest-bayesian-optimisation.html\n",
    "\n",
    "If you look at their examples you'll find that they only have the function maximize, nothing on minimize. This means if we use RMSE or something we would not get something useful. Hence, there are two ways that could be think of. One, implement the Ariel Score as we want to maximize that. Second, use \"negative (root) mean squared error\". This way, it could be maximize as well. \n",
    "\n",
    "After deciding, `neg_mean_squared_error` would be a good choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning hyperparameters for tree-based learners. Based on Datacamp course on Intro to XGBoost. \n",
    "\n",
    "- Learning rate (eta). \n",
    "- Gamma: min loss reduction to create new tree split. \n",
    "- Lambda: (int) L2 regularization\n",
    "- Alpha: (int) L1 regularization\n",
    "- max_depth: (positive intger) how deep can a tree grows. \n",
    "- subsample: (0, 1]. Fraction of total training set that can be used for any given boosting round. Low means little amount of training data used, but may lead to underfitting. High might mean overfitting. \n",
    "- colsample_bytree: (0, 1]. The fraction of **features** that it can be used (selected) from during any given boosting round. Large value means (almost) all features can be used to build a tree. Smaller is additional regularization by restricting number of features. Using all columns might result in overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_cv(X, y, target=\"label\", **kwargs):\n",
    "    \"\"\"\n",
    "    XGBoost Regressor Cross Validation Function.\n",
    "\n",
    "    Parameters: \n",
    "        :var dataset: (Pandas.DataFrame) A Pandas DataFrame of our used dataset. \n",
    "        :var target: (str) The column name of the target. Default to \"label\". \n",
    "        :var kwargs: (dict) A dictionary for the optimizer to pass in as params for XGBoost\n",
    "                Regressor. \n",
    "    \"\"\"\n",
    "    kwargs[\"objective\"] = \"reg:squarederror\"\n",
    "    estimator = xgb.XGBRegressor(**kwargs)\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "\n",
    "    # Using 4-fold validation. \n",
    "    cval = cross_val_score(estimator, X, y, scoring=\"neg_mean_squared_error\",\n",
    "                            cv=3)\n",
    "\n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimization(dataset, target, parameters, n_iter=10, init_points=3):\n",
    "    \"\"\"\n",
    "    Bayesian Optimization Algorithm. \n",
    "\n",
    "    Parameters:\n",
    "        :var dataset: (Pandas.DataFrame) A Pandas DataFrame of our used dataset. \n",
    "        :var target: (str) The column name of the target. Default to \"label\".\n",
    "        :var parameters: (Python Dict) The dictionary containing the parameters (or its range) to\n",
    "                optimize on. \n",
    "        :var n_iter: (int) How many steps of Bayesian Optimization to go through. The more steps \n",
    "                the more likely to find a good maximum. \n",
    "        :var init_points: (int) How many steps of random exploration to perform. Random \n",
    "                exploration can help in diversifying the exploration space. \n",
    "        :var **kwargs: other BayesianOptimization.maximize() parameters. \n",
    "    \"\"\"\n",
    "    y = dataset.pop(target)\n",
    "    X = dataset\n",
    "    \n",
    "    def our_crossval(**kwargs):\n",
    "        \"\"\"\n",
    "        Wrapper function for cross validation. \n",
    "\n",
    "        This might requires us to ensure casting of integer is correct, values passed in are   \n",
    "        correct, etc. This includes capping to (0, 1) range for learning rate. \n",
    "        \"\"\"\n",
    "        return xgb_cv(\n",
    "            X, y, target,\n",
    "            learning_rate = max(min(kwargs[\"learning_rate\"], 0.999), 1e-4),\n",
    "            reg_lambda = int(kwargs[\"reg_lambda\"]),\n",
    "            reg_alpha = int(kwargs[\"reg_alpha\"]),\n",
    "            gamma = int(kwargs[\"gamma\"]),\n",
    "            subsample = max(min(kwargs[\"subsample\"], 0.999), 1e-3),\n",
    "            colsample_bytree = max(min(kwargs[\"colsample_bytree\"], 0.999), 1e-3),\n",
    "            max_depth = 15,\n",
    "            n_estimators = 25,\n",
    "        )\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f = our_crossval,\n",
    "        pbounds = parameters,\n",
    "        verbose = 2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(n_iter=n_iter)\n",
    "\n",
    "    return optimizer.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of now I haven't know if there can be fixed values passed in as parameters to the bounds, as the bayesian optimization official github page does not have examples on such, so we will continue as such. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"learning_rate\": (1e-4, 0.3),\n",
    "    \"reg_lambda\": (1, 100),\n",
    "    \"reg_alpha\": (1, 100),\n",
    "    \"gamma\": (1, 100),\n",
    "    \"subsample\": (0.3, 0.999),\n",
    "    \"colsample_bytree\": (1e-3, 0.999),\n",
    "}\n",
    "\n",
    "target = \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up bigquery reading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "\n",
    "try:\n",
    "    bqclient = bigquery.Client()\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "except:\n",
    "    print(\"Using backup credentials.\")\n",
    "    \n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        \"/workspace/ariel_ml_2021/sunlit-analyst-309609-77b8e2f94cb5.json\",\n",
    "        scopes = [\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "    )\n",
    "\n",
    "    bqclient = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient(credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would only try for one file here to see if it runs. Then we would integrate this into real work later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bigquery df --use_bqstorage_api\n",
    "# SELECT * EXCEPT (AAAA, BB, CC)\n",
    "# FROM `sunlit-analyst-309609.training_set.train_table_19` a\n",
    "# LEFT JOIN (\n",
    "#     SELECT *\n",
    "#     FROM `sunlit-analyst-309609.training_set.noisy_train_extra_params`\n",
    "# ) b\n",
    "# ON a.AAAA = b.AAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for table_num in range(55):\n",
    "#     query_string = f\"\"\"\n",
    "#     SELECT * EXCEPT (AAAA, BB, CC)\n",
    "#     FROM `sunlit-analyst-309609.training_set.train_table_{table_num}`\n",
    "#     \"\"\"\n",
    "    \n",
    "#     df = (\n",
    "#         bqclient.query(query_string)\n",
    "#         .result()\n",
    "#         .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "#     )\n",
    "    \n",
    "#     print(\"Starting Bayesian Optimization\")\n",
    "    \n",
    "#     returned_value = bayesian_optimization(df, target=target, parameters=parameters, n_iter=15)\n",
    "    \n",
    "#     with open(\"target.txt\", \"a\") as f:\n",
    "#         f.write(f\"Table number {table_num}\\n\")\n",
    "#         f.write(str(returned_value))\n",
    "#         f.write(\"\\n\\n\")\n",
    "    \n",
    "#     print(\"Finished with table \", table_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "\n",
    "Find the parameters that give the lowest score, and do one more time cross validation with that parameters (for those with either not that parameter or having the target neg mean squared error too high to be considered ideal). Then, also include the original values as a choice (that is, default value), and then cross validate and see the score. Finally, compare both score with the original. If it is better than use that score, if it is worse we will stay with the tuned score. \n",
    "\n",
    "Write this into a new .txt file **with only the training parameters**. This will allow for indexing using numpy arrays loadtxt method. We are going to read from that file and convert it with `ast.literal_evals` into dict and pass the values into `XGBRegressor`  for training. **Don't forget to make integer values that are supposed to be integer before passing in**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files from target.txt\n",
    "\n",
    "test_df = []\n",
    "\n",
    "with open(\"target.txt\") as f:\n",
    "    test_df.append(f.readlines())\n",
    "\n",
    "test_df = np.squeeze(np.array(test_df))\n",
    "\n",
    "test_np = []\n",
    "for ii in np.arange(1, len(test_df), 3):\n",
    "    test_np.append(test_df[ii].strip(\"\\n\"))\n",
    "\n",
    "test_np = np.array(test_np)\n",
    "\n",
    "test_np = np.array([ast.literal_eval(test_np[ii]) for ii in np.arange(len(test_np))])\n",
    "\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in test_np:\n",
    "    assert type(ii) is dict\n",
    "\n",
    "assert len(test_np) == 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0007101904942502508"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_np[0][\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: what should be int should be int, what should be limit between 0 and 1 should do\n",
    "# so, before input into tuning as kwargs. \n",
    "\n",
    "def preprocessing(test_np):\n",
    "\n",
    "    best_target_score = -999\n",
    "    memory_loc = 0\n",
    "\n",
    "    for ii in range(len(test_np)):\n",
    "        target = test_np[ii][\"target\"]\n",
    "        params = test_np[ii][\"params\"]\n",
    "\n",
    "        params[\"learning_rate\"] = max(min(params[\"learning_rate\"], 0.999), 1e-4)\n",
    "        params[\"reg_lambda\"] = int(params[\"reg_lambda\"])\n",
    "        params[\"reg_alpha\"] = int(params[\"reg_alpha\"])\n",
    "        params[\"gamma\"] = int(params[\"gamma\"])\n",
    "        params[\"subsample\"] = max(min(params[\"subsample\"], 0.999), 1e-3)\n",
    "        params[\"colsample_bytree\"] = max(min(params[\"colsample_bytree\"], 0.999), 1e-3)\n",
    "        params[\"max_depth\"] = 15\n",
    "        params[\"n_estimators\"] = 25\n",
    "\n",
    "        # check for 'best' target score: \n",
    "        if target > best_target_score:\n",
    "            best_target_score = target\n",
    "            memory_loc = ii  # Use as retrieving value\n",
    "\n",
    "        test_np[ii][\"params\"] = params\n",
    "\n",
    "    return test_np, best_target_score, memory_loc\n",
    "\n",
    "\n",
    "test_np, best_target_score, memory_loc = preprocessing(test_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below have not been updated and the for loop have not been taken out. Howver, basically, trying to run with the default parameter takes too long to return, hence it is abandoned. Even if it does better, the time it took does not worth training for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Table 0.\n",
      "\n",
      "0   -0.0007101904942502508\n",
      "1   -0.00027482415665553854\n",
      "2   -0.00027482415665553854\n",
      "\n",
      "Finished with table  0 \n",
      "\n",
      "Starting Table 1.\n",
      "\n",
      "0   -0.0009665722530871574\n",
      "1   -0.00021376560262598097\n",
      "2   -0.00020741055420593906\n",
      "\n",
      "Finished with table  1 \n",
      "\n",
      "Starting Table 2.\n",
      "\n",
      "0   -0.0001822130862766251\n",
      "1   -0.00017607126787741394\n",
      "2   -0.00017607126787741394\n",
      "\n",
      "Finished with table  2 \n",
      "\n",
      "Starting Table 3.\n",
      "\n",
      "0   -0.00022833661895986835\n",
      "1   -0.00021124457970168254\n",
      "2   -0.00020550071766618698\n",
      "\n",
      "Finished with table  3 \n",
      "\n",
      "Starting Table 4.\n",
      "\n",
      "0   -0.00028027326374360594\n",
      "1   -0.0001930955149589792\n",
      "2   -0.0001930955149589792\n",
      "\n",
      "Finished with table  4 \n",
      "\n",
      "Starting Table 5.\n",
      "\n",
      "0   -0.0007783382394149252\n",
      "1   -0.0001938505222129354\n",
      "2   -0.00018778580500383341\n",
      "\n",
      "Finished with table  5 \n",
      "\n",
      "Starting Table 6.\n",
      "\n",
      "0   -0.000221691282695802\n",
      "1   -0.000185230125222218\n",
      "2   -0.000185230125222218\n",
      "\n",
      "Finished with table  6 \n",
      "\n",
      "Starting Table 7.\n",
      "\n",
      "0   -0.0006023584212557878\n",
      "1   -0.00019514841144583373\n",
      "2   -0.00018953467970741017\n",
      "\n",
      "Finished with table  7 \n",
      "\n",
      "Starting Table 8.\n",
      "\n",
      "0   -0.00036792525953116296\n",
      "1   -0.00021338167375032092\n",
      "2   -0.00021338167375032092\n",
      "\n",
      "Finished with table  8 \n",
      "\n",
      "Starting Table 9.\n",
      "\n",
      "0   -0.0008061170901870858\n",
      "1   -0.00035710722492228583\n",
      "2   -0.0003488108255840648\n",
      "\n",
      "Finished with table  9 \n",
      "\n",
      "Starting Table 10.\n",
      "\n",
      "0   -0.0005600670166634052\n",
      "1   -0.0003652012023077509\n",
      "2   -0.0003652012023077509\n",
      "\n",
      "Finished with table  10 \n",
      "\n",
      "Starting Table 11.\n",
      "\n",
      "0   -0.000449546893728701\n",
      "1   -0.0003415055078594624\n",
      "2   -0.00033607369074479805\n",
      "\n",
      "Finished with table  11 \n",
      "\n",
      "Starting Table 12.\n",
      "\n",
      "0   -0.0009276328678956429\n",
      "1   -0.00033879365516337564\n",
      "2   -0.00033879365516337564\n",
      "\n",
      "Finished with table  12 \n",
      "\n",
      "Starting Table 13.\n",
      "\n",
      "0   -0.000766093881354782\n",
      "1   -0.00035699556597052997\n",
      "2   -0.0003517884642624065\n",
      "\n",
      "Finished with table  13 \n",
      "\n",
      "Starting Table 14.\n",
      "\n",
      "0   -0.00043115244398193154\n",
      "1   -0.00035063385912466566\n",
      "2   -0.00035063385912466566\n",
      "\n",
      "Finished with table  14 \n",
      "\n",
      "Starting Table 15.\n",
      "\n",
      "0   -0.00040159467577579347\n",
      "1   -0.00034993156646551224\n",
      "2   -0.0003443673415989991\n",
      "\n",
      "Finished with table  15 \n",
      "\n",
      "Starting Table 16.\n",
      "\n",
      "0   -0.0011262803012154514\n",
      "1   -0.00035243840152069777\n",
      "2   -0.00035243840152069777\n",
      "\n",
      "Finished with table  16 \n",
      "\n",
      "Starting Table 17.\n",
      "\n",
      "0   -0.0002644014793873913\n",
      "1   -0.00036773905891652846\n",
      "2   -0.0002644020831641089\n",
      "\n",
      "Finished with table  17 \n",
      "\n",
      "Starting Table 18.\n",
      "\n",
      "0   -0.00045414405114337146\n",
      "1   -0.0003639956013059031\n",
      "2   -0.00035849960631503455\n",
      "\n",
      "Finished with table  18 \n",
      "\n",
      "Starting Table 19.\n",
      "\n",
      "0   -0.0014069734000942324\n",
      "1   -0.0003732217928761199\n",
      "2   -0.0003732217928761199\n",
      "\n",
      "Finished with table  19 \n",
      "\n",
      "Starting Table 20.\n",
      "\n",
      "0   -0.0005584825130150479\n",
      "1   -0.00039146057953343925\n",
      "2   -0.00038390202649859014\n",
      "\n",
      "Finished with table  20 \n",
      "\n",
      "Starting Table 21.\n",
      "\n",
      "0   -0.0006332102069810286\n",
      "1   -0.0003838797699393279\n",
      "2   -0.0003838797699393279\n",
      "\n",
      "Finished with table  21 \n",
      "\n",
      "Starting Table 22.\n",
      "\n",
      "0   -0.0015646132867986752\n",
      "1   -0.0003978762293103028\n",
      "2   -0.0003865779478230969\n",
      "\n",
      "Finished with table  22 \n",
      "\n",
      "Starting Table 23.\n",
      "\n",
      "0   -0.0005385820987052442\n",
      "1   -0.0004034243659832471\n",
      "2   -0.0004034243659832471\n",
      "\n",
      "Finished with table  23 \n",
      "\n",
      "Starting Table 24.\n",
      "\n",
      "0   -0.0015210958734239285\n",
      "1   -0.00039240484146966354\n",
      "2   -0.0003842730094574907\n",
      "\n",
      "Finished with table  24 \n",
      "\n",
      "Starting Table 25.\n",
      "\n",
      "0   -0.0004388535930324876\n",
      "1   -0.0004299975200294591\n",
      "2   -0.0004299975200294591\n",
      "\n",
      "Finished with table  25 \n",
      "\n",
      "Starting Table 26.\n",
      "\n",
      "0   -0.0005657910790045486\n",
      "1   -0.0004494272410822005\n",
      "2   -0.00043700889702066734\n",
      "\n",
      "Finished with table  26 \n",
      "\n",
      "Starting Table 27.\n",
      "\n",
      "0   -0.0010684847206627256\n",
      "1   -0.0004137591004576516\n",
      "2   -0.0004137591004576516\n",
      "\n",
      "Finished with table  27 \n",
      "\n",
      "Starting Table 28.\n",
      "\n",
      "0   -0.0007124266252888376\n",
      "1   -0.00041393266794674755\n",
      "2   -0.0004003196799796684\n",
      "\n",
      "Finished with table  28 \n",
      "\n",
      "Starting Table 29.\n",
      "\n",
      "0   -0.001588512044127735\n",
      "1   -0.000427613449252058\n",
      "2   -0.000427613449252058\n",
      "\n",
      "Finished with table  29 \n",
      "\n",
      "Starting Table 30.\n",
      "\n",
      "0   -0.0009860890606513638\n",
      "1   -0.0004260491983992334\n",
      "2   -0.00041112105959922396\n",
      "\n",
      "Finished with table  30 \n",
      "\n",
      "Starting Table 31.\n",
      "\n",
      "0   -0.0013449964309707914\n",
      "1   -0.00041804900826755156\n",
      "2   -0.00041804900826755156\n",
      "\n",
      "Finished with table  31 \n",
      "\n",
      "Starting Table 32.\n",
      "\n",
      "0   -0.0004931278644917294\n",
      "1   -0.00044486889617454864\n",
      "2   -0.0004319487115539114\n",
      "\n",
      "Finished with table  32 \n",
      "\n",
      "Starting Table 33.\n",
      "\n",
      "0   -0.0010617182149247509\n",
      "1   -0.00042976531468796165\n",
      "2   -0.00042976531468796165\n",
      "\n",
      "Finished with table  33 \n",
      "\n",
      "Starting Table 34.\n",
      "\n",
      "0   -0.0006272058967355452\n",
      "1   -0.00045822515231215186\n",
      "2   -0.0004335731350403392\n",
      "\n",
      "Finished with table  34 \n",
      "\n",
      "Starting Table 35.\n",
      "\n",
      "0   -0.0007432673857970196\n",
      "1   -0.00044755705114131054\n",
      "2   -0.00044755705114131054\n",
      "\n",
      "Finished with table  35 \n",
      "\n",
      "Starting Table 36.\n",
      "\n",
      "0   -0.0013059654826694759\n",
      "1   -0.0004991551363528026\n",
      "2   -0.0004741118871964944\n",
      "\n",
      "Finished with table  36 \n",
      "\n",
      "Starting Table 37.\n",
      "\n",
      "0   -0.0007113745095362959\n",
      "1   -0.0004823977763644708\n",
      "2   -0.0004823977763644708\n",
      "\n",
      "Finished with table  37 \n",
      "\n",
      "Starting Table 38.\n",
      "\n",
      "0   -0.0010056332084405672\n",
      "1   -0.0005091432749156568\n",
      "2   -0.00048077667783071157\n",
      "\n",
      "Finished with table  38 \n",
      "\n",
      "Starting Table 39.\n",
      "\n",
      "0   -0.0013924587839293388\n",
      "1   -0.0004793069760653835\n",
      "2   -0.0004793069760653835\n",
      "\n",
      "Finished with table  39 \n",
      "\n",
      "Starting Table 40.\n",
      "\n",
      "0   -0.0009928725395538865\n",
      "1   -0.0005103958565120421\n",
      "2   -0.00047982382885855475\n",
      "\n",
      "Finished with table  40 \n",
      "\n",
      "Starting Table 41.\n",
      "\n",
      "0   -0.001391786909563794\n",
      "1   -0.0005404292862835573\n",
      "2   -0.0005404292862835573\n",
      "\n",
      "Finished with table  41 \n",
      "\n",
      "Starting Table 42.\n",
      "\n",
      "0   -0.000467425843674306\n",
      "1   -0.0006927895650796877\n",
      "2   -0.00046740218567626316\n",
      "\n",
      "Finished with table  42 \n",
      "\n",
      "Starting Table 43.\n",
      "\n",
      "0   -0.000936128572902287\n",
      "1   -0.001031993719386778\n",
      "2   -0.000936128572902287\n",
      "\n",
      "Finished with table  43 \n",
      "\n",
      "Starting Table 44.\n",
      "\n",
      "0   -0.00034388787454757994\n",
      "1   -0.0003424435305126175\n",
      "2   -0.00033476239250142893\n",
      "\n",
      "Finished with table  44 \n",
      "\n",
      "Starting Table 45.\n",
      "\n",
      "0   -0.0012036715839668355\n",
      "1   -0.00042035016026100125\n",
      "2   -0.00042035016026100125\n",
      "\n",
      "Finished with table  45 \n",
      "\n",
      "Starting Table 46.\n",
      "\n",
      "0   -0.0011883443981607759\n",
      "1   -0.00040882278464038775\n",
      "2   -0.0004007170696406364\n",
      "\n",
      "Finished with table  46 \n",
      "\n",
      "Starting Table 47.\n",
      "\n",
      "0   -0.0005109047181384511\n",
      "1   -0.00037715307622585926\n",
      "2   -0.00037715307622585926\n",
      "\n",
      "Finished with table  47 \n",
      "\n",
      "Starting Table 48.\n",
      "\n",
      "0   -0.0004220903816150769\n",
      "1   -0.00042440631836525034\n",
      "2   -0.0004071643095974458\n",
      "\n",
      "Finished with table  48 \n",
      "\n",
      "Starting Table 49.\n",
      "\n",
      "0   -0.0011140924957017352\n",
      "1   -0.00045053242014627906\n",
      "2   -0.0004245984175948706\n",
      "\n",
      "Finished with table  49 \n",
      "\n",
      "Starting Table 50.\n",
      "\n",
      "0   -0.0013585133086766517\n",
      "1   -0.00041599812017110684\n",
      "2   -0.00041599812017110684\n",
      "\n",
      "Finished with table  50 \n",
      "\n",
      "Starting Table 51.\n",
      "\n",
      "0   -0.0010037155319742319\n",
      "1   -0.00048461072086527563\n",
      "2   -0.00045912459301084615\n",
      "\n",
      "Finished with table  51 \n",
      "\n",
      "Starting Table 52.\n",
      "\n",
      "0   -0.0005674675378571107\n",
      "1   -0.0005798319541621671\n",
      "2   -0.0005674473386942755\n",
      "\n",
      "Finished with table  52 \n",
      "\n",
      "Starting Table 53.\n",
      "\n",
      "0   -0.0006293007617256243\n",
      "1   -0.0005961328607482453\n",
      "2   -0.0005961328607482453\n",
      "\n",
      "Finished with table  53 \n",
      "\n",
      "Starting Table 54.\n",
      "\n",
      "0   -0.0009867981868895153\n",
      "1   -0.001263404748013082\n",
      "2   -0.0009868220240935996\n",
      "\n",
      "Finished with table  54 \n",
      "\n",
      "CPU times: user 8h 1min 26s, sys: 2min 6s, total: 8h 3min 33s\n",
      "Wall time: 1h 27min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Try best target score parameters, default parameters, and assigned parameters. \n",
    "# Then for the same parameter, try whether doubling n_estimators improves training.\n",
    "\n",
    "for table_num in range(len(test_np)):\n",
    "    query_string = f\"\"\"\n",
    "    SELECT * EXCEPT (AAAA, BB, CC)\n",
    "    FROM `sunlit-analyst-309609.training_set.train_table_{table_num}`\n",
    "    \"\"\"\n",
    "    \n",
    "    df = (\n",
    "        bqclient.query(query_string)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "\n",
    "    y = df.pop(target)\n",
    "    X = df\n",
    "\n",
    "    print(f\"Starting Table {table_num}.\\n\")\n",
    "\n",
    "    # Try first objective: We can skip best target score as we already computed it.\n",
    "    kwargs = [test_np[table_num][\"params\"], test_np[memory_loc][\"params\"]]\n",
    "    curr_best = test_np[table_num][\"target\"]\n",
    "    best = 0\n",
    "\n",
    "    print(0, \" \", curr_best)\n",
    "\n",
    "    i = 1\n",
    "    \n",
    "    curr_mean = xgb_cv(X, y, \"label\", **kwargs[i])\n",
    "\n",
    "    if curr_mean > curr_best:\n",
    "        best = i\n",
    "        curr_best = curr_mean\n",
    "\n",
    "    print(i, \" \", curr_mean)\n",
    "\n",
    "    kwargs = kwargs[best]\n",
    "\n",
    "    # Second objective: does doubling n_estimators improve training? \n",
    "    kwargs[\"n_estimators\"] = 50\n",
    "    curr_mean = xgb_cv(X, y, \"label\", **kwargs)\n",
    "    print(i + 1, \" \", curr_mean)\n",
    "\n",
    "    if curr_mean > curr_best:\n",
    "        curr_best = curr_mean\n",
    "    else:\n",
    "        kwargs[\"n_estimators\"] = 25\n",
    "\n",
    "    # Write curr best to file: \n",
    "    with open(\"second_target.txt\", \"a\") as f:\n",
    "        f.write(f\"Table number {table_num}\\n\")\n",
    "        f.write(str(kwargs))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "    print(\"\\nFinished with table \", table_num, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own testing before bayesian optimization to make sure it run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = f\"\"\"\n",
    "SELECT * EXCEPT (AAAA, BB, CC)\n",
    "FROM `sunlit-analyst-309609.training_set.train_table_19`\n",
    "\"\"\"\n",
    "\n",
    "df = (\n",
    "    bqclient.query(query_string)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = copy.deepcopy(df)\n",
    "\n",
    "y = m.pop(\"label\")\n",
    "X = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:09:01] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[12:09:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 108 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[12:09:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 170 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:09:49] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 460 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:10:17] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 686 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:10:48] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 842 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:11:22] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 988 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:11:57] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1098 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:12:30] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1248 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:13:06] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1308 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:13:48] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1506 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:14:29] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1486 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:15:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1630 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:15:56] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1680 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:16:40] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1606 extra nodes, 0 pruned nodes, max_depth=10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=10,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=15, n_jobs=16, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=2)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_param = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"max_depth\": 25,\n",
    "    \"subsample\": 0.2,\n",
    "    \"colsample_bytree\": 0.3,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=15, max_depth=10, verbosity=2)\n",
    "\n",
    "xg_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = X, y\n",
    "\n",
    "preds = xg_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.00866371649129658\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be done\n",
    "\n",
    "Add `n_estimators` as a tuning parameter to xgboost? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0251145 , 0.0338163 , 0.0292234 , ..., 0.04004366, 0.03397508,\n",
       "       0.03676901], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.025124\n",
       "1         0.025124\n",
       "2         0.025124\n",
       "3         0.025124\n",
       "4         0.025124\n",
       "            ...   \n",
       "125595    0.021874\n",
       "125596    0.021874\n",
       "125597    0.021874\n",
       "125598    0.021874\n",
       "125599    0.021874\n",
       "Name: label, Length: 125600, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
