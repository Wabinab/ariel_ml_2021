{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381064bit3810pyenvc587e5b1529f4bbdabdcafa5a0673238",
   "display_name": "Python 3.8.10 64-bit ('3.8.10': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Creating XGBoost model. \n",
    "\n",
    "This is based on how we trained 55 separate models for 55 separate wavelengths. \n",
    "\n",
    "Note that the difference, other than using XGBoost model, is that Decision Trees does **not** require normalizing data. Hence we can go as it is. \n",
    "\n",
    "And since our data is pre-cleaned, we also do not need to put a pipeline into it. \n",
    "\n",
    "It is a plus that Decision Trees (and hence XGBoost) works best when the features are a collection of categorical and numerical features, OR purely numerical features, which the latter is for ours. \n",
    "\n",
    "And it's a plus if the number of features is far less than the number of training samples. We can drop features as well later during training randomly. \n",
    "\n",
    "However, since we are not familiar with XGBoost, and we have lots of features, tuning it is something of a requirement due to inexperience. We would do bayesian optimization for ourselves. Even though wandb offers pre-configured and easily sent job, we could learn more by implementing ourselves plus I have no idea how to retrieve best parameters from Weights and Biases. \n",
    "\n",
    "Also note that there's a chance we might not use the whole dataset for hparams tuning if it takes too long. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: GCLOUD_PROJECT=sunlit-analyst-309609\n"
     ]
    }
   ],
   "source": [
    "storage_name = \"baseline_xgboost_pred_1.txt\"\n",
    "\n",
    "PROJECT_ID = \"sunlit-analyst-309609\"\n",
    "%env GCLOUD_PROJECT = $PROJECT_ID\n",
    "%load_ext google.cloud.bigquery\n",
    "\n",
    "!export GOOGLE_APPLICATION_CREDENTIALS=\"/workspace/ariel_ml_2021/sunlit-analyst-309609-77b8e2f94cb5.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tempfile\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import xgboost as xgb \n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from google.cloud import bigquery, bigquery_storage\n",
    "LOCATION = \"us\""
   ]
  },
  {
   "source": [
    "Examples taken from https://github.com/fmfn/BayesianOptimization/blob/master/examples/sklearn_example.py and https://www.kdnuggets.com/2019/07/xgboost-random-forest-bayesian-optimisation.html\n",
    "\n",
    "If you look at their examples you'll find that they only have the function maximize, nothing on minimize. This means if we use RMSE or something we would not get something useful. Hence, there are two ways that could be think of. One, implement the Ariel Score as we want to maximize that. Second, use \"negative (root) mean squared error\". This way, it could be maximize as well. \n",
    "\n",
    "After deciding, `neg_mean_squared_error` would be a good choice. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Tuning hyperparameters for tree-based learners. Based on Datacamp course on Intro to XGBoost. \n",
    "\n",
    "- Learning rate (eta). \n",
    "- Gamma: min loss reduction to create new tree split. \n",
    "- Lambda: (int) L2 regularization\n",
    "- Alpha: (int) L1 regularization\n",
    "- max_depth: (positive intger) how deep can a tree grows. \n",
    "- subsample: (0, 1]. Fraction of total training set that can be used for any given boosting round. Low means little amount of training data used, but may lead to underfitting. High might mean overfitting. \n",
    "- colsample_bytree: (0, 1]. The fraction of **features** that it can be used (selected) from during any given boosting round. Large value means (almost) all features can be used to build a tree. Smaller is additional regularization by restricting number of features. Using all columns might result in overfitting. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_cv(X, y, target=\"label\", **kwargs):\n",
    "    \"\"\"\n",
    "    XGBoost Regressor Cross Validation Function.\n",
    "\n",
    "    Parameters: \n",
    "        :var dataset: (Pandas.DataFrame) A Pandas DataFrame of our used dataset. \n",
    "        :var target: (str) The column name of the target. Default to \"label\". \n",
    "        :var kwargs: (dict) A dictionary for the optimizer to pass in as params for XGBoost\n",
    "                Regressor. \n",
    "    \"\"\"\n",
    "    kwargs[\"objective\"] = \"reg:squarederror\"\n",
    "    estimator = xgb.XGBRegressor(params = kwargs)\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "\n",
    "    # Using 4-fold validation. \n",
    "    cval = cross_val_score(estimator, X, y, scoring=\"neg_mean_squared_error\",\n",
    "                            cv=4)\n",
    "\n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimization(dataset, target, parameters, n_iter=10, init_points=3):\n",
    "    \"\"\"\n",
    "    Bayesian Optimization Algorithm. \n",
    "\n",
    "    Parameters:\n",
    "        :var dataset: (Pandas.DataFrame) A Pandas DataFrame of our used dataset. \n",
    "        :var target: (str) The column name of the target. Default to \"label\".\n",
    "        :var parameters: (Python Dict) The dictionary containing the parameters (or its range) to\n",
    "                optimize on. \n",
    "        :var n_iter: (int) How many steps of Bayesian Optimization to go through. The more steps \n",
    "                the more likely to find a good maximum. \n",
    "        :var init_points: (int) How many steps of random exploration to perform. Random \n",
    "                exploration can help in diversifying the exploration space. \n",
    "        :var **kwargs: other BayesianOptimization.maximize() parameters. \n",
    "    \"\"\"\n",
    "    y = dataset.pop(target)\n",
    "    X = dataset\n",
    "    \n",
    "    def our_crossval(**kwargs):\n",
    "        \"\"\"\n",
    "        Wrapper function for cross validation. \n",
    "\n",
    "        This might requires us to ensure casting of integer is correct, values passed in are   \n",
    "        correct, etc. This includes capping to (0, 1) range for learning rate. \n",
    "        \"\"\"\n",
    "        return xgb_cv(\n",
    "            X, y, target,\n",
    "            learning_rate = max(min(kwargs[\"learning_rate\"], 0.999), 1e-4),\n",
    "            reg_lambda = int(kwargs[\"reg_lambda\"]),\n",
    "            reg_alpha = int(kwargs[\"reg_alpha\"]),\n",
    "            gamma = int(kwargs[\"gamma\"]),\n",
    "            max_depth = int(kwargs[\"max_depth\"]),\n",
    "            subsample = max(min(kwargs[\"subsample\"], 0.999), 1e-3),\n",
    "            colsample_bytree = max(min(kwargs[\"colsample_bytree\"], 0.999), 1e-3),\n",
    "        )\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f = our_crossval,\n",
    "        pbounds = parameters,\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(n_iter=n_iter)\n",
    "\n",
    "    return optimizer.max"
   ]
  },
  {
   "source": [
    "As of now I haven't know if there can be fixed values passed in as parameters to the bounds, as the bayesian optimization official github page does not have examples on such, so we will continue as such. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"learning_rate\": (1e-4, 0.3),\n",
    "    \"reg_lambda\": (1, 1000),\n",
    "    \"reg_alpha\": (1, 1000),\n",
    "    \"gamma\": (1, 1000),\n",
    "    \"max_depth\": (1, 25),\n",
    "    \"subsample\": (1e-3, 0.999),\n",
    "    \"colsample_bytree\": (1e-3, 0.999),\n",
    "}\n",
    "\n",
    "target = \"label\""
   ]
  },
  {
   "source": [
    "Set up bigquery reading. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"/workspace/ariel_ml_2021/sunlit-analyst-309609-77b8e2f94cb5.json\",\n",
    "    scopes = [\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "bqclient = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "bqstorageclient = bigquery_storage.BigQueryReadClient(credentials=credentials)"
   ]
  },
  {
   "source": [
    "We would only try for one file here to see if it runs. Then we would integrate this into real work later on. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bigquery df --use_bqstorage_api\n",
    "# SELECT * EXCEPT (AAAA, BB, CC)\n",
    "# FROM `sunlit-analyst-309609.training_set.train_table_19` a\n",
    "# LEFT JOIN (\n",
    "#     SELECT *\n",
    "#     FROM `sunlit-analyst-309609.training_set.noisy_train_extra_params`\n",
    "# ) b\n",
    "# ON a.AAAA = b.AAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = f\"\"\"\n",
    "SELECT * EXCEPT (AAAA, BB, CC)\n",
    "FROM `sunlit-analyst-309609.training_set.train_table_19`\n",
    "\"\"\"\n",
    "\n",
    "df = (\n",
    "    bqclient.query(query_string)\n",
    "    .result()\n",
    "    .to_dataframe(bqstorage_client=bqstorageclient)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\", line 191, in probe\n",
      "    target = self._cache[_hashable(x)]\n",
      "KeyError: (0.7838390843112499, 336.60789118492244, 0.12195455990607709, 17.16334692765465, 508.45330966595867, 459.1508871987576, 0.8795131357941941)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 711, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 725, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 526, in __init__\n",
      "    raise TypeError(\"Input data can not be a list.\")\n",
      "TypeError: Input data can not be a list.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\", line 191, in probe\n",
      "    target = self._cache[_hashable(x)]\n",
      "KeyError: (0.7838390843112499, 336.60789118492244, 0.12195455990607709, 17.16334692765465, 508.45330966595867, 459.1508871987576, 0.8795131357941941)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 711, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 725, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 526, in __init__\n",
      "    raise TypeError(\"Input data can not be a list.\")\n",
      "TypeError: Input data can not be a list.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\", line 191, in probe\n",
      "    target = self._cache[_hashable(x)]\n",
      "KeyError: (0.7838390843112499, 336.60789118492244, 0.12195455990607709, 17.16334692765465, 508.45330966595867, 459.1508871987576, 0.8795131357941941)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 711, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 725, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 526, in __init__\n",
      "    raise TypeError(\"Input data can not be a list.\")\n",
      "TypeError: Input data can not be a list.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\", line 191, in probe\n",
      "    target = self._cache[_hashable(x)]\n",
      "KeyError: (0.2523363952532722, 344.4867193257332, 0.05981767082047908, 10.832330248545503, 229.53414880885114, 987.6036335824222, 0.9328556747368412)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 711, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 725, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 526, in __init__\n",
      "    raise TypeError(\"Input data can not be a list.\")\n",
      "TypeError: Input data can not be a list.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\", line 191, in probe\n",
      "    target = self._cache[_hashable(x)]\n",
      "KeyError: (0.2523363952532722, 344.4867193257332, 0.05981767082047908, 10.832330248545503, 229.53414880885114, 987.6036335824222, 0.9328556747368412)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 711, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 725, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 526, in __init__\n",
      "    raise TypeError(\"Input data can not be a list.\")\n",
      "TypeError: Input data can not be a list.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\", line 191, in probe\n",
      "    target = self._cache[_hashable(x)]\n",
      "KeyError: (0.2523363952532722, 344.4867193257332, 0.05981767082047908, 10.832330248545503, 229.53414880885114, 987.6036335824222, 0.9328556747368412)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 711, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 725, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 526, in __init__\n",
      "    raise TypeError(\"Input data can not be a list.\")\n",
      "TypeError: Input data can not be a list.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\", line 191, in probe\n",
      "    target = self._cache[_hashable(x)]\n",
      "KeyError: (0.2523363952532722, 344.4867193257332, 0.05981767082047908, 10.832330248545503, 229.53414880885114, 987.6036335824222, 0.9328556747368412)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 711, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 725, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 526, in __init__\n",
      "    raise TypeError(\"Input data can not be a list.\")\n",
      "TypeError: Input data can not be a list.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\", line 191, in probe\n",
      "    target = self._cache[_hashable(x)]\n",
      "KeyError: (0.8635881736062565, 181.48974077653358, 0.04066734229167697, 5.70472203632734, 211.9060919739337, 683.4687130267804, 0.1780342016551412)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 711, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 725, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 526, in __init__\n",
      "    raise TypeError(\"Input data can not be a list.\")\n",
      "TypeError: Input data can not be a list.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\", line 191, in probe\n",
      "    target = self._cache[_hashable(x)]\n",
      "KeyError: (0.8635881736062565, 181.48974077653358, 0.04066734229167697, 5.70472203632734, 211.9060919739337, 683.4687130267804, 0.1780342016551412)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 711, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 725, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 526, in __init__\n",
      "    raise TypeError(\"Input data can not be a list.\")\n",
      "TypeError: Input data can not be a list.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\", line 191, in probe\n",
      "    target = self._cache[_hashable(x)]\n",
      "KeyError: (0.8635881736062565, 181.48974077653358, 0.04066734229167697, 5.70472203632734, 211.9060919739337, 683.4687130267804, 0.1780342016551412)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 711, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 725, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 526, in __init__\n",
      "    raise TypeError(\"Input data can not be a list.\")\n",
      "TypeError: Input data can not be a list.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\", line 191, in probe\n",
      "    target = self._cache[_hashable(x)]\n",
      "KeyError: (0.8635881736062565, 181.48974077653358, 0.04066734229167697, 5.70472203632734, 211.9060919739337, 683.4687130267804, 0.1780342016551412)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 711, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 236, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/sklearn.py\", line 725, in <lambda>\n",
      "    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 436, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/workspace/.pip-modules/lib/python3.8/site-packages/xgboost/core.py\", line 526, in __init__\n",
      "    raise TypeError(\"Input data can not be a list.\")\n",
      "TypeError: Input data can not be a list.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.9251944830507636, 594.0396412028109, 0.14312579091674452, 18.11050911079083, 359.41761379775846, 245.01354392326402, 0.4023744250417942)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b6518c0f224e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreturned_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbayesian_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturned_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-30890d6f1dc3>\u001b[0m in \u001b[0;36mbayesian_optimization\u001b[0;34m(dataset, target, parameters, n_iter, init_points)\u001b[0m\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-30890d6f1dc3>\u001b[0m in \u001b[0;36mour_crossval\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metc\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0mcapping\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \"\"\"\n\u001b[0;32m---> 26\u001b[0;31m         return xgb_cv(\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-9042082d85c8>\u001b[0m in \u001b[0;36mxgb_cv\u001b[0;34m(X, y, target, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Using 4-fold validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     cval = cross_val_score(estimator, X.to_dict(\"records\"), y, scoring=\"neg_mean_squared_error\",\n\u001b[0m\u001b[1;32m     18\u001b[0m                             cv=4)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_dict\u001b[0;34m(self, orient, into)\u001b[0m\n\u001b[1;32m   1598\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m             )\n\u001b[0;32m-> 1600\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m   1601\u001b[0m                 \u001b[0minto_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_box_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1598\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m             )\n\u001b[0;32m-> 1600\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m   1601\u001b[0m                 \u001b[0minto_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_box_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1595\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m             rows = (\n\u001b[0;32m-> 1597\u001b[0;31m                 \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1598\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m             )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "returned_value = bayesian_optimization(copy.deepcopy(df), target=target, parameters=parameters)\n",
    "print(returned_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = copy.deepcopy(df)\n",
    "\n",
    "y = m.pop(\"label\")\n",
    "X = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[12:09:01] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[12:09:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 108 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[12:09:27] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 170 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:09:49] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 460 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:10:17] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 686 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:10:48] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 842 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:11:22] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 988 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:11:57] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1098 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:12:30] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1248 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:13:06] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1308 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:13:48] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1506 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:14:29] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1486 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:15:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1630 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:15:56] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1680 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[12:16:40] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 1606 extra nodes, 0 pruned nodes, max_depth=10\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=10,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=15, n_jobs=16, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=2)"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "my_param = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"max_depth\": 25,\n",
    "    \"subsample\": 0.2,\n",
    "    \"colsample_bytree\": 0.3,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=15, max_depth=10, verbosity=2)\n",
    "\n",
    "xg_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = X, y\n",
    "\n",
    "preds = xg_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE: 0.00866371649129658\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "source": [
    "# To be done\n",
    "\n",
    "Add `n_estimators` as a tuning parameter to xgboost? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.0251145 , 0.0338163 , 0.0292234 , ..., 0.04004366, 0.03397508,\n",
       "       0.03676901], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0         0.025124\n",
       "1         0.025124\n",
       "2         0.025124\n",
       "3         0.025124\n",
       "4         0.025124\n",
       "            ...   \n",
       "125595    0.021874\n",
       "125596    0.021874\n",
       "125597    0.021874\n",
       "125598    0.021874\n",
       "125599    0.021874\n",
       "Name: label, Length: 125600, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}