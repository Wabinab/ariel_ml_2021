{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381064bit3810pyenvc587e5b1529f4bbdabdcafa5a0673238",
   "display_name": "Python 3.8.10 64-bit ('3.8.10': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Creating XGBoost model. \n",
    "\n",
    "This is based on how we trained 55 separate models for 55 separate wavelengths. \n",
    "\n",
    "Note that the difference, other than using XGBoost model, is that Decision Trees does **not** require normalizing data. Hence we can go as it is. \n",
    "\n",
    "And since our data is pre-cleaned, we also do not need to put a pipeline into it. \n",
    "\n",
    "It is a plus that Decision Trees (and hence XGBoost) works best when the features are a collection of categorical and numerical features, OR purely numerical features, which the latter is for ours. \n",
    "\n",
    "And it's a plus if the number of features is far less than the number of training samples. We can drop features as well later during training randomly. \n",
    "\n",
    "However, since we are not familiar with XGBoost, and we have lots of features, tuning it is something of a requirement due to inexperience. We would do bayesian optimization for ourselves. Even though wandb offers pre-configured and easily sent job, we could learn more by implementing ourselves plus I have no idea how to retrieve best parameters from Weights and Biases. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: GCLOUD_PROJECT=sunlit-analyst-309609\n"
     ]
    }
   ],
   "source": [
    "storage_name = \"baseline_xgboost_pred_1.txt\"\n",
    "\n",
    "PROJECT_ID = \"sunlit-analyst-309609\"\n",
    "%env GCLOUD_PROJECT = $PROJECT_ID\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import xgboost as xgb \n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from google.cloud import bigquery\n",
    "LOCATION = \"us\""
   ]
  },
  {
   "source": [
    "Examples taken from https://github.com/fmfn/BayesianOptimization/blob/master/examples/sklearn_example.py and https://www.kdnuggets.com/2019/07/xgboost-random-forest-bayesian-optimisation.html\n",
    "\n",
    "If you look at their examples you'll find that they only have the function maximize, nothing on minimize. This means if we use RMSE or something we would not get something useful. Hence, there are two ways that could be think of. One, implement the Ariel Score as we want to maximize that. Second, use \"negative (root) mean squared error\". This way, it could be maximize as well. \n",
    "\n",
    "After deciding, `neg_mean_squared_error` would be a good choice. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimization(dataset, function, parameters, target=\"label\", n_iter=10, init_points=3):\n",
    "    \"\"\"\n",
    "    Bayesian Optimization Algorithm. \n",
    "\n",
    "    Parameters:\n",
    "    :var dataset: (Pandas.DataFrame) A Pandas DataFrame of our used dataset. \n",
    "    :var function: (Python Function) Function containing our model to optimize on. \n",
    "    :var parameters: (Python Dict) The dictionary containing the parameters (or its range) to\n",
    "            optimize on. \n",
    "    :var target: (str) The column name of the target. Default to \"label\". \n",
    "    :var n_iter: (int) How many steps of Bayesian Optimization to go through. The more steps the\n",
    "            more likely to find a good maximum. \n",
    "    :var init_points: (int) How many steps of random exploration to perform. Random exploration\n",
    "            can help in diversifying the exploration space. \n",
    "    :var **kwargs: other BayesianOptimization.maximize() parameters. \n",
    "    \"\"\"\n",
    "    y = dataset.pop(target)\n",
    "    X = dataset\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  }
 ]
}