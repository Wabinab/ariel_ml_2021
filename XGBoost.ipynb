{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381064bit3810pyenvc587e5b1529f4bbdabdcafa5a0673238",
   "display_name": "Python 3.8.10 64-bit ('3.8.10': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Creating XGBoost model. \n",
    "\n",
    "This is based on how we trained 55 separate models for 55 separate wavelengths. \n",
    "\n",
    "Note that the difference, other than using XGBoost model, is that Decision Trees does **not** require normalizing data. Hence we can go as it is. \n",
    "\n",
    "And since our data is pre-cleaned, we also do not need to put a pipeline into it. \n",
    "\n",
    "It is a plus that Decision Trees (and hence XGBoost) works best when the features are a collection of categorical and numerical features, OR purely numerical features, which the latter is for ours. \n",
    "\n",
    "And it's a plus if the number of features is far less than the number of training samples. We can drop features as well later during training randomly. \n",
    "\n",
    "However, since we are not familiar with XGBoost, and we have lots of features, tuning it is something of a requirement due to inexperience. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: GCLOUD_PROJECT=sunlit-analyst-309609\n"
     ]
    }
   ],
   "source": [
    "storage_name = \"baseline_xgboost_pred_1.txt\"\n",
    "\n",
    "PROJECT_ID = \"sunlit-analyst-309609\"\n",
    "%env GCLOUD_PROJECT = $PROJECT_ID\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import xgboost as xgb \n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from google.cloud import bigquery\n",
    "LOCATION = \"us\""
   ]
  },
  {
   "source": [
    "The Bayesian Optimization technique had been taken from https://www.kdnuggets.com/2019/07/xgboost-random-forest-bayesian-optimisation.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimization(dataset, function, parameters):\n",
    "    "
   ]
  }
 ]
}